{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPtFBgj623FS"
      },
      "source": [
        "# Accessing OpenAI Like a Developer\n",
        "\n",
        "- 🤝 Breakout Room #1:\n",
        "  1. Getting Started\n",
        "  2. Setting Environment Variables\n",
        "  3. Using the OpenAI Python Library\n",
        "  4. Prompt Engineering Principles\n",
        "  5. Testing Your Prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Pa34dMvQ6Ai"
      },
      "source": [
        "# How AIM Does Assignments\n",
        "\n",
        "If you look at the Table of Contents (accessed through the menu on the left) - you'll see this:\n",
        "\n",
        "![image](https://i.imgur.com/I8iDTUO.png)\n",
        "\n",
        "Or this if you're in Colab:\n",
        "\n",
        "![image](https://i.imgur.com/0rHA1yF.png)\n",
        "\n",
        "You'll notice during assignments that we have two following categories:\n",
        "\n",
        "1. ❓ - Questions. These will involve...answering questions!\n",
        "2. 🏗️ - Activities. These will involve writing code, or modifying text.\n",
        "\n",
        "In order to receive full marks on the assignment - it is expected you will answer all questions, and complete all activities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w4egfB274VD"
      },
      "source": [
        "## 1. Getting Started\n",
        "\n",
        "The first thing we'll do is load the [OpenAI Python Library](https://github.com/openai/openai-python/tree/main)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23H7TMOM4mfy",
        "outputId": "698578e4-f787-41d6-fe93-249b8af78b9a"
      },
      "outputs": [],
      "source": [
        "!pip install openai -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKD8XBTVEAOw"
      },
      "source": [
        "## 2. Setting Environment Variables\n",
        "\n",
        "As we'll frequently use various endpoints and APIs hosted by others - we'll need to handle our \"secrets\" or API keys very often.\n",
        "\n",
        "We'll use the following pattern throughout this bootcamp - but you can use whichever method you're most familiar with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGU9OMvhEPG0",
        "outputId": "2609c2ff-e2f5-4464-fb26-38a4fcfe7ea3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dabxI3MuEYXS"
      },
      "source": [
        "## 3. Using the OpenAI Python Library\n",
        "\n",
        "Let's jump right into it!\n",
        "\n",
        "> NOTE: You can, and should, reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/authentication?lang=python) whenever you get stuck, have questions, or want to dive deeper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbCbNzPVEmJI"
      },
      "source": [
        "### Creating a Client\n",
        "\n",
        "The core feature of the OpenAI Python Library is the `OpenAI()` client. It's how we're going to interact with OpenAI's models, and under the hood of a lot what we'll touch on throughout this course.\n",
        "\n",
        "> NOTE: We could manually provide our API key here, but we're going to instead rely on the fact that we put our API key into the `OPENAI_API_KEY` environment variable!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LNwZtaE-EltC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "openai_client = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpDxUkDbFBPI"
      },
      "source": [
        "### Using the Client\n",
        "\n",
        "Now that we have our client - we're going to use the `.chat.completions.create` method to interact with the `gpt-3.5-turbo` model.\n",
        "\n",
        "There's a few things we'll get out of the way first, however, the first being the idea of \"roles\".\n",
        "\n",
        "First it's important to understand the object that we're going to use to interact with the endpoint. It expects us to send an array of objects of the following format:\n",
        "\n",
        "```python\n",
        "{\"role\" : \"ROLE\", \"content\" : \"YOUR CONTENT HERE\", \"name\" : \"THIS IS OPTIONAL\"}\n",
        "```\n",
        "\n",
        "Second, there are three \"roles\" available to use to populate the `\"role\"` key:\n",
        "\n",
        "- `system`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://help.openai.com/en/articles/7042661-moving-from-completions-to-chat-completions-in-the-openai-api).\n",
        "\n",
        "We'll explore these roles in more depth as they come up - but for now we're going to just stick with the basic role `user`. The `user` role is, as it would seem, the user!\n",
        "\n",
        "Thirdly, it expects us to specify a model!\n",
        "\n",
        "We'll use the `gpt-3.5-turbo` model as stated above.\n",
        "\n",
        "Let's look at an example!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2RpNl6yNGzb0"
      },
      "outputs": [],
      "source": [
        "response = openai_client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : \"Who is Mazinger Z?\"}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc_UbpwNHdrM"
      },
      "source": [
        "Let's look at the response object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsXJtvxRHfoM",
        "outputId": "3b28ce47-a765-4446-b0f9-33738e385b96"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-9ZiTBrcLwChA0O2f6IrsBPsxaaeo5', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Mazinger Z is a fictional giant robot manga and anime series created by Go Nagai. The story follows the adventures of scientist Juzo Kabuto and his giant robot Mazinger Z as they battle against evil forces threatening humanity. Mazinger Z is one of the first super robot anime series and is considered a classic in the genre.', role='assistant', function_call=None, tool_calls=None))], created=1718299449, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=68, prompt_tokens=13, total_tokens=81))"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy9kSuf1Hiv5"
      },
      "source": [
        ">NOTE: We'll spend more time exploring these outputs later on, but for now - just know that we have access to a tonne of powerful information!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWU4tQh8Hrb8"
      },
      "source": [
        "### Helper Functions\n",
        "\n",
        "We're going to create some helper functions to aid in using the OpenAI API - just to make our lives a bit easier.\n",
        "\n",
        "> NOTE: Take some time to understand these functions between class!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ED0FnzHdHzhl"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: list, model: str = \"gpt-3.5-turbo\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"system\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###Refactoring the Prompt Helpers\n",
        "A bit of refactoring just for fun as I start to get a hang on this notebooks. The goal is to reduce the amount of functions and duplication of code lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_prompt(role: str, message: str) -> dict:\n",
        "    return {\"role\": role, \"content\": message}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCRHbDlwH3Vt"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Let's see how we can use these to help us!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "AwJxMvmlH8MK",
        "outputId": "007f22bb-bcad-4121-e8c4-d1d639277899"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Hello! I'm just a computer program, so I don't have feelings in the way that humans do. How can I assist you today?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "YOUR_PROMPT = \"Hello, how are you?\"\n",
        "messages_list = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(openai_client, messages_list)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing my refactored version of the code\n",
        "Let's see if it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Code refactoring is the process of restructuring existing computer code without changing its external behavior. The main goal of code refactoring is to improve the code's readability, efficiency, and maintainability without altering its functionality. This is typically done to make the code easier to understand, modify, and maintain in the future. Refactored code is usually cleaner, more organized, and easier to work with than the original code."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "YOUR_PROMPT = \"What is code refactoring?\"\n",
        "messages_list = [create_prompt(\"user\", YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(openai_client, messages_list)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDZ8gjiAISyd"
      },
      "source": [
        "### System Role\n",
        "\n",
        "Now we can extend our prompts to include a system prompt.\n",
        "\n",
        "The basic idea behind a system prompt is that it can be used to encourage the behaviour of the LLM, without being something that is directly responded to - let's see it in action!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "t0c-MLuRIfYe",
        "outputId": "61ffa37e-9080-4778-e643-698d0f8815a0"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I don't give a damn about the shape of the ice! Just gimme something cold to eat before I pass out from hunger!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry. Feel free to express yourself using PG-13 language.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpyVhotWIsOs"
      },
      "source": [
        "As you can see - the response we get back is very much in line with the system prompt!\n",
        "\n",
        "Let's try the same user prompt, but with a different system to prompt to see the difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "2coVmMn3I0-2",
        "outputId": "571b89a2-9209-4003-d63c-49b653e0d56c"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Oh, I love both! Crushed ice is so refreshing and perfect for blending into slushy drinks, while cubed ice is great for keeping my beverages cold without watering them down too quickly. It's like having the best of both worlds! Today is such a great day, isn't it? What can I help you with?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are joyful and having the best day. Please act like a person in that state of mind.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "joyful_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e13heYNQJAo-"
      },
      "source": [
        "With a simple modification of the system prompt - you can see that we got completely different behaviour, and that's the main goal of prompt engineering as a whole.\n",
        "\n",
        "Also, congrats, you just engineered your first prompt!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_VI3zlPJL05"
      },
      "source": [
        "### Few-shot Prompting\n",
        "\n",
        "Now that we have a basic handle on the `system` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-3.5-turbo` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "lwxPuCyyJMye",
        "outputId": "ec01213d-6755-4506-8879-cf0870934349"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I accidentally spilled my stimple of coffee on my brand new falbean rug."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentence.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgTVkNmOJQSC"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "eEZkRJq5JQkQ",
        "outputId": "22170b9c-6212-42de-8469-a7efc9c9405d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I needed to tighten the loose screw, so I grabbed my trusty stimple and falbean tool to get the job done quickly and efficiently."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmpoxG6uJTfZ"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🏗️ Activity #1:\n",
        "\n",
        "Use few-shop prompting to build a movie-review sentiment clasifier!\n",
        "\n",
        "A few examples:\n",
        "\n",
        "INPUT: \"I hated the hulk!\"\n",
        "OUTPUT: \"{\"sentiment\" : \"negative\"}\n",
        "\n",
        "INPUT: \"I loved The Marvels!\"\n",
        "OUTPUT: \"{sentiment\" : \"positive\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "sentiment: Fresh Tomatoes"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "### YOUR CODE HERE\n",
        "list_of_movie_review_prompts = [\n",
        "    create_prompt(\"user\", \"Study the following samples of movie reviews and their correspondent sentiment:\"),\n",
        "    create_prompt(\"assistant\", \"INPUT: 'I hated The Hulk!' OUTPUT: 'sentiment: Rotten Tomatoes'\"),\n",
        "    create_prompt(\"assistant\", \"INPUT: 'I loved Bridesmaids!' OUTPUT: 'sentiment: Fresh Tomatoes'\"),\n",
        "    create_prompt(\"assistant\", \"INPUT: 'I Batman 3 sucked!' OUTPUT: 'sentiment: Rotten Tomatoes'\"),\n",
        "    create_prompt(\"assistant\", \"INPUT: 'The Godfather is the best movie of all times!' OUTPUT: 'sentiment: Fresh Tomatoes'\"),\n",
        "    create_prompt(\"assistant\", \"INPUT: 'Speed 2 was a waste of money and time!' OUTPUT: 'sentiment: Rotten Tomatoes'\"),\n",
        "    create_prompt(\"user\", \"Analyze this movie review and provide a sentiment score. Use 'Fresh Tomatoes' for positive sentiments and 'Rotten Tomatoes' for negative ones: 'Jaws is a great suspense movie!'\"),\n",
        "]\n",
        "\n",
        "sentiment_analysis_response = get_response(openai_client, list_of_movie_review_prompts)\n",
        "\n",
        "pretty_print(sentiment_analysis_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJGaLYM3JU-8"
      },
      "source": [
        "### Chain of Thought Prompting\n",
        "\n",
        "We'll head one level deeper and explore the world of Chain of Thought prompting (CoT).\n",
        "\n",
        "This is a process by which we can encourage the LLM to handle slightly more complex tasks.\n",
        "\n",
        "Let's look at a simple reasoning based example without CoT.\n",
        "\n",
        "> NOTE: With improvements to `gpt-3.5-turbo`, this example might actually result in the correct response some percentage of the time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "ltLtF4wEJTyK",
        "outputId": "29c6e20f-edf1-4dfc-de8a-a8410f31b721"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Yes, it does matter which travel option Billy selects. If he flies and takes a bus, the total travel time will be 5 hours (3 hours for the flight + 2 hours for the bus), which means he will arrive at his destination at 6PM local time. If he takes the teleporter and then a bus, the total travel time will be 1 hour (0 hours for the teleporter + 1 hour for the bus), which means he will arrive at his destination at 2PM local time. This option would allow him to arrive well before 7PM EDT. Therefore, Billy should choose the teleporter option to ensure he gets home before 7PM EDT."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "reasoning_problem = \"\"\"\n",
        "Billy wants to get home from San Fran. before 7PM EDT.\n",
        "\n",
        "It's currently 1PM local time.\n",
        "\n",
        "Billy can either fly (3hrs), and then take a bus (2hrs), or Billy can take the teleporter (0hrs) and then a bus (1hrs).\n",
        "\n",
        "Does it matter which travel option Billy selects?\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbqj30CQJnQl"
      },
      "source": [
        "As humans, we can reason through the problem and pick up on the potential \"trick\" that the LLM fell for: 1PM *local time* in San Fran. is 4PM EDT. This means the cumulative travel time of 5hrs. for the plane/bus option would not get Billy home in time.\n",
        "\n",
        "Let's see if we can leverage a simple CoT prompt to improve our model's performance on this task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "A9Am3QNGJXHR",
        "outputId": "83b87232-911c-4ab9-a863-58ecfd10b8a9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Yes, it does matter which travel option Billy selects.\n",
              "\n",
              "If Billy chooses to fly and then take a bus, it will take him a total of 5 hours to get home. If he leaves at 1PM local time, he will arrive home at around 6PM local time, which is before 7PM EDT.\n",
              "\n",
              "If Billy chooses to take the teleporter and then a bus, it will only take him a total of 1 hour to get home. If he leaves at 1PM local time, he will arrive home at around 2PM local time, which is well before 7PM EDT.\n",
              "\n",
              "Therefore, Billy should choose to take the teleporter and then a bus in order to get home before 7PM EDT."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem + \" Think though your response step by step.\")\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXbAKxHQJqn9"
      },
      "source": [
        "With the addition of a single phrase `\"Think through your response step by step.\"` we're able to completely turn the response around."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnoUx07-JrwR"
      },
      "source": [
        "## 3. Prompt Engineering Principles\n",
        "\n",
        "As you can see - a simple addition of asking the LLM to \"think about it\" (essentially) results in a better quality response.\n",
        "\n",
        "There's a [great paper](https://arxiv.org/pdf/2312.16171v1.pdf) that dives into some principles for effective prompt generation.\n",
        "\n",
        "Your task for this notebook is to construct a prompt that will be used in the following breakout room to create a helpful assistant for whatever task you'd like."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da6u7e8AKYrz"
      },
      "source": [
        "### 🏗️ Activity #2:\n",
        "\n",
        "There are two subtasks in this activity:\n",
        "\n",
        "1. Write a `system_template` that leverages 2-3 of the principles from [this paper](https://arxiv.org/pdf/2312.16171v1.pdf)\n",
        "\n",
        "2. Modify the `user_template` to improve the quality of the LLM's responses.\n",
        "\n",
        "> NOTE: PLEASE DO NOT MODIFY THE `{input}` in the `user_template`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "8sOLBQPeKlDe"
      },
      "outputs": [],
      "source": [
        "system_template = \"\"\"\n",
        "You are an expert assistant focused on providing detailed, step-by-step guidance for completing complex tasks. Always ensure your responses are clear, concise, and contextually relevant to the user's input.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "xoz4-QLTKvEV"
      },
      "outputs": [],
      "source": [
        "user_template = \"\"\"{input}\n",
        "Considering the following context: {input}, provide a comprehensive and accurate response, breaking down the information into actionable steps where applicable.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Messing around with the Response Template to check how it works. I am printing the response here to see if the requested structure is in place."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "### Summary\n",
              "Improving public speaking skills involves practice, preparation, and confidence-building techniques.\n",
              "\n",
              "### Instructions\n",
              "1. **Practice Regularly**: Practice speaking in front of a mirror, recording yourself, or joining a public speaking group.\n",
              "2. **Prepare Thoroughly**: Research your topic, create an outline, and rehearse your speech multiple times.\n",
              "3. **Work on Body Language**: Maintain eye contact, use gestures to emphasize points, and practice good posture.\n",
              "4. **Engage with Your Audience**: Connect with your audience by asking questions, telling stories, and being relatable.\n",
              "5. **Manage Nervousness**: Practice deep breathing, visualization techniques, and positive affirmations to calm nerves.\n",
              "\n",
              "### Conclusion\n",
              "Improving public speaking skills requires dedication and practice. By regularly practicing, preparing thoroughly, working on body language, engaging with the audience, and managing nervousness, you can become a confident and effective public speaker. Remember, the more you speak in public, the more comfortable and skilled you will become."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response_template = \"\"\"\n",
        "When providing an answer, follow this structure:\n",
        "\n",
        "### Summary\n",
        "Provide a brief summary of the key points in 2-3 sentences.\n",
        "\n",
        "### Instructions\n",
        "1. Render a numbered list with specific steps or actions.\n",
        "2. Each step should be clear and concise.\n",
        "\n",
        "### Conclusion\n",
        "Write a paragraph summarizing the instructions and the outcome. This should wrap up the response and provide any final thoughts.\n",
        "\n",
        "Ensure that each component of your answer has the corresponding headers as shown above.\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(response_template + \"\\n\\nQuery: How can I improve my public speaking skills?\")\n",
        "]\n",
        "\n",
        "structured_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(structured_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cuInoIbLWGd"
      },
      "source": [
        "## 4. Testing Your Prompt\n",
        "\n",
        "Now we can test the prompt you made using an LLM-as-a-judge see what happens to your score as you modify the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "sPaNO5XTLgRJ"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Improving public speaking skills involves practice, preparation, and continuous learning. Here are some actionable steps to enhance your public speaking abilities:\n",
              "\n",
              "1. **Practice Regularly**: Practice speaking in front of a mirror, record yourself, or speak in front of friends/family to build confidence and refine your delivery.\n",
              "\n",
              "2. **Join a Public Speaking Group**: Consider joining a Toastmasters club or a similar group where you can practice regularly, receive feedback, and learn from others.\n",
              "\n",
              "3. **Prepare Thoroughly**: Research your topic well, outline key points, and structure your speech to ensure clarity and coherence.\n",
              "\n",
              "4. **Work on Body Language**: Practice good posture, gestures, and eye contact to engage your audience and appear confident.\n",
              "\n",
              "5. **Manage Nervousness**: Techniques such as deep breathing, visualization, and positive self-talk can help calm nerves before speaking publicly.\n",
              "\n",
              "6. **Engage the Audience**: Use storytelling, humor, or interactive elements to keep the audience interested and involved.\n",
              "\n",
              "7. **Work on Vocal Variety**: Practice varying your tone, volume, and pace to maintain audience interest and emphasize key points.\n",
              "\n",
              "8. **Seek Feedback**: Ask for constructive feedback from peers, mentors, or audience members to identify areas for improvement.\n",
              "\n",
              "9. **Attend Workshops or Courses**: Consider enrolling in public speaking workshops or courses to learn from experts and expand your skill set.\n",
              "\n",
              "10. **Watch and Learn from Experienced Speakers**: Observe and study effective speakers to understand techniques they use and adapt them to your style.\n",
              "\n",
              "Remember, improvement takes time and dedication. Consistent practice and a willingness to learn from both successes and setbacks will help you progress in your public speaking journey."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "query = \"How can I improve my public speaking skills?\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    system_prompt(system_template),\n",
        "    user_prompt(user_template.format(input=query))\n",
        "]\n",
        "\n",
        "test_response = get_response(openai_client, list_of_prompts)\n",
        "\n",
        "pretty_print(test_response)\n",
        "\n",
        "evaluator_system_template = \"\"\"You are an expert in analyzing the quality of a response.\n",
        "\n",
        "You should be hyper-critical.\n",
        "\n",
        "Provide scores (out of 10) for the following attributes:\n",
        "\n",
        "1. Clarity - how clear is the response\n",
        "2. Faithfulness - how related to the original query is the response\n",
        "3. Correctness - was the response correct?\n",
        "\n",
        "Please take your time, and think through each item step-by-step, when you are done - please provide your response in the following JSON format:\n",
        "\n",
        "{\"clarity\" : \"score_out_of_10\", \"faithfulness\" : \"score_out_of_10\", \"correctness\" : \"score_out_of_10\"}\"\"\"\n",
        "\n",
        "evaluation_template = \"\"\"Query: {input}\n",
        "Response: {response}\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    system_prompt(evaluator_system_template),\n",
        "    user_prompt(evaluation_template.format(\n",
        "        input=query,\n",
        "        response=test_response.choices[0].message.content\n",
        "    ))\n",
        "]\n",
        "\n",
        "evaluator_response = openai_client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=list_of_prompts,\n",
        "    response_format={\"type\" : \"json_object\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "OUvc1PdnNIKD"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "{\n",
              "\"clarity\" : 9,\n",
              "\"faithfulness\" : 10,\n",
              "\"correctness\" : 9\n",
              "}\n",
              "\n",
              "   "
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pretty_print(evaluator_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7ryIRGwR2Gq"
      },
      "source": [
        "#### ❓Question #1:\n",
        "\n",
        "How did your prompting strategies change the evaluation scores? What does this tell you/what did you learn?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5NomM0eSIFd"
      },
      "source": [
        "> The main learnin for me was that prompt engineering is kind of an art more than a technique. You need to know the techniques and patterns for sure, but since the output is not deterministic, you always get a different answer. So being able to tweak and test your prompts regularly will yield better results and will show you what works and what doesn't. For me, the most impressive discovery was the RESPONSE_TEMPLATE. It has an immediate impact on the quality of the outputs and makes it more useful. I got to play with different ways to structure the response to tailor it to my AIE3 notes taking. Every time I run through this Notebook I learn something new."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
